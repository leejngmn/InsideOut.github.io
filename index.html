
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="InsideOut: Integrated RGB-Radiative Gaussian Splatting for ComprehSensive 3D Object Representation">
  <meta name="keywords" content="3D Gaussian splatting, novel view synthesis">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>InsideOut</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>  
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateInteractive();">

<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">InsideOut: Integrated RGB-Radiative Gaussian Splatting for ComprehSensive 3D Object Representation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://leejngmn.github.io/">Jungmin Lee</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="">Seonghyuk Hong</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="">Juyong Lee</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="">Jaeyoon Lee</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="">Jongwon Choi*</a><sup>1, 2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>VILab, Chung-Ang University</span>
            <span class="author-block"><sup>2</sup>Cultural Heritage Conservation Science Center, National Research Institute of Cultural Heritage</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="https://openaccess.thecvf.com/content/ICCV2025/papers/InsideOut_ICCV_2025_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

            <!-- Data Link. -->
              <span class="link-block">
                <a target="_blank" href="https:/f"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-data"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>  
              
            <!-- Video Link. -->
            <span class="link-block">
              <a target="_blank" href=""
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>

            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href=""
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="rows">
          <div style="text-align: center;">
            <img src="images/framework.gng" class="interpolation-image" style="width: 100%; height: auto;"/>
          </div>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
          <b>What if 3D objects could show both surface details and hidden interior simultaneously?</b>
          <br>We explore two distinct imaging modalities that capture complementary aspects of the world: RGB imaging reveals rich surface textures and colors, while X-ray imaging penetrates through materials to expose internal structures. These modalities are naturally complementary but have never been unified in 3D reconstruction. InsideOut, bridges gap by extending 3D Gaussian Splatting to fuse RGB and X-ray data, enabling unprecedented visualization capabilities for medical diagnostics, cultural heritage analysis, and manufacturing inspection.
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce InsideOut, an extension of 3D Gaussian splatting (3DGS) that bridges the gap between high-fidelity RGB surface details and subsurface X-ray structures. The fusion of RGB and X-ray imaging is invaluable in fields such as medical diagnostics, cultural heritage restoration, and manufacturing. We collect new paired RGB and X-ray data, perform hierarchical fitting to align RGB and X-ray radiative Gaussian splats, and propose an X-ray reference loss to ensure consistent internal structures. InsideOut effectively addresses the challenges posed by disparate data representations between the two modalities and limited paired datasets. This approach significantly extends the applicability of 3DGS, enhancing visualization, simulation, and non-destructive testing capabilities across various domains.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  <!-- Paper video. -->
  <!-- <br>
  <br>

  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3">Video</h2>
      <div class="publication-video">
        <iframe src="https://www.youtube.com/embed/Yvn4eR05A3M"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
    </div>
  </div> -->

</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3">Application</h2>

      <h2 class="title is-5">Real-world Asset Diagnosis</h2>
      <p class="content has-text-justified">
        The quality of a caption can be evaluated by assessing the image reconstruction produced by a text-to-image model. The best text for an image is one that leads to the most accurate reconstruction of the original image
      </p>

      <p class="content has-text-justified">
      <b>Method</b>: The generated image is compared with the input image using a similarity function based on CLIP image embeddings.
      Human-annotated captions serve as ground truth representations of the input image to evaluate the quality of the generated caption.      
      </p>

      <div style="text-align: center;">
        <img src="images/teaser.png" class="interpolation-image" style="width: 60%; height: auto;"/>
      </div>
      <br>

      <b>Evaluation</b>: The result with three similarity and four caption metrics on the NoCaps dataset suggests that regardless of the similarity or evaluation metric, better image reconstruction always leads to better captions.
      </p>
      <br>

      <div style="text-align: center;">
        <img src="images/Page5.png" class="interpolation-image" style="width: 60%; height: auto;"/>
      </div>
      <br>
      
      <p class="content has-text-justified">
      <b>Qualitative examples</b> of reconstruction tasks. The figure shows image reconstruction for the given input image of a bird. Two samples are shown with their generated images, ranked by the image similarity.
      </p>

      <div style="text-align: center;">
        <img src="images/Page3.png" class="interpolation-image" style="width: 50%; height: auto;"/>
      </div>
    </br>


      <h2 class="title is-5">Text-to-Image-to-Text</h2>
      <p class="content has-text-justified">
        The quality of a generated image can be evaluated by assessing the text reconstruction produced by a caption model. The best image for an image is one that leads to the most accurate reconstruction of the original text.
      </p>

      <p class="content has-text-justified">
        <b>Method</b>: The generated image is compared with the input image using a similarity function based on CLIP image embeddings.
        Human-annotated captions serve as ground truth representations of the input image to evaluate the quality of the generated caption.      
        </p>
  
        <div style="text-align: center;">
          <img src="images/Page6.png" class="interpolation-image" style="width: 60%; height: auto;"/>
        </div>
        <br>
  
        <b>Evaluation</b>: The result with three similarity and four caption metrics on the NoCaps dataset suggests that regardless of the similarity or evaluation metric, better image reconstruction always leads to better captions.
        </p>
        <br>
  
        <div style="text-align: center;">
          <img src="images/Page7.png" class="interpolation-image" style="width: 60%; height: auto;"/>
        </div>
        <br>
        
        <p class="content has-text-justified">
        <b>Qualitative examples</b> of reconstruction tasks. The figure shows text reconstruction for the given input image of a text. Two samples are shown with their generated texts, ranked by the image similarity. 
        </p>
  
        <div style="text-align: center;">
          <img src="images/Page8.png" class="interpolation-image" style="width: 50%; height: auto;"/>
        </div>
  
        <br>
        <p class="content has-text-justified">
          <b>More Visualizations:</b> 
          </p>
        <div style="text-align: center;">
          <img src="images/Page11.jpg" class="interpolation-image" style="width: 100%; height: auto;"/>
        </div>

        <br>
        <div style="text-align: center;">
          <img src="images/Page12.jpg" class="interpolation-image" style="width: 100%; height: auto;"/>
        </div>

    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3">Cycle Finetuning</h2>
      <p class="content has-text-justified">
      <b>A Finetuning Framework</b> We propose a unified framework to enhance both the image-to-text and text-to-image models: finetuning the image-to-text model using a reconstruction loss computed by a text-to-image model as regularization.
      </p>
      <div style="text-align: center;">
        <img src="images/Page2.png" class="interpolation-image" style="width: 120%; height: auto;"/>
      </div>

      <br>
      <p class="content has-text-justified">
        <b>Improvement:</b> Image-to-Text Captioning task
        </p>
        <div style="text-align: center;">
          <img src="images/Page9.png" class="interpolation-image" style="width: 120%; height: auto;"/>
        </div>

        <b>Improvement:</b> Text-to-Image Generation task
      </p>
      <br>
      <div style="text-align: center;">
        <img src="images/Page10.png" class="interpolation-image" style="width: 50%; height: auto;"/>
      </div>
    
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@InProceedings{,
        author    = {},
        title     = {InsideOut: Integrated RGB-Radiative Gaussian Splatting for ComprehSensive 3D Object Representation},
        booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
        month     = {October},
        year      = {2025},
        pages     = {}
    }</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a href="https://dalleflamingo.github.io/">DALLE Flamingo</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
